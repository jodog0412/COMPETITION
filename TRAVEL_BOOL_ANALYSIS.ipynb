{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRAVEL_BOOL_ANALYSIS.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv5hCb8T+A5RfBqbb7fTw4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jodog0412/DACON/blob/main/TRAVEL_BOOL_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "brUgcRpFxtsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBidw9kXDlVl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab/TRAVEL_PRODUCT\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "train = pd.read_csv('dataset/train.csv')\n",
        "test = pd.read_csv('dataset/test.csv')\n",
        "sample_submission = pd.read_csv('dataset/sample_submission.csv')\n",
        "train.info()\n",
        "train.isna().sum()"
      ],
      "metadata": {
        "id": "v7GRxH_XD_Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 처리\n",
        "def handle_na(data):\n",
        "    temp = data.copy()\n",
        "    for col, dtype in temp.dtypes.items():\n",
        "        if dtype == 'object':\n",
        "            value = 'unknown'\n",
        "        elif dtype == int or dtype == float:\n",
        "            value = 0\n",
        "        temp.loc[:,col] = temp[col].fillna(value)\n",
        "    return temp\n",
        "train_nona=handle_na(train)\n",
        "test_nona = handle_na(test)\n",
        "train_nona.info()\n",
        "test_nona.info()"
      ],
      "metadata": {
        "id": "RIb9eMvbEesq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = train_nona.columns[train_nona.dtypes == 'object']\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "train_enc = train_nona.copy()\n",
        "test_enc=test_nona.copy()\n",
        "for o_col in object_columns:\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(train_enc[o_col])\n",
        "    train_enc[o_col] = encoder.transform(train_enc[o_col])\n",
        "    test_enc[o_col] = encoder.transform(test_enc[o_col])\n",
        "\n",
        "train_enc.info()\n",
        "train_enc.describe(include=\"number\")\n",
        "# train_enc[\"MonthlyIncome\"].hist(bins=100)"
      ],
      "metadata": {
        "id": "fw-f_CB4EZ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "x_train = train_enc.drop(columns=['id','ProdTaken'])\n",
        "x_test = test_enc.drop(columns=['id'])\n",
        "y_train = train_enc[['ProdTaken']]\n",
        "\n",
        "clf = make_pipeline(RobustScaler(), SVC(gamma='auto'))\n",
        "clf.fit(x_train, y_train)\n",
        "prediction=clf.predict(x_test)\n",
        "cm = confusion_matrix(y_train, prediction[:1955]>0.5)\n",
        "cm\n",
        "# sample_submission['ProdTaken'] = prediction\n",
        "# sample_submission.to_csv('submission.csv',index = False)\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch import nn\n",
        "# from torch import optim\n",
        "# import matplotlib.pyplot as plt\n",
        "# torch.manual_seed(1)\n",
        "\n",
        "# X=torch.tensor(x_train.values)\n",
        "# Y=torch.tensor(y_train.values)\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(18, 36),\n",
        "#     nn.LeakyReLU(),\n",
        "#     nn.Linear(36, 18),\n",
        "#     nn.LeakyReLU(),\n",
        "#     nn.Linear(18, 9),\n",
        "#     nn.LeakyReLU(),\n",
        "#     nn.Linear(9, 4), \n",
        "#     nn.LeakyReLU(),\n",
        "#     nn.Linear(4, 1) \n",
        "# )\n",
        "# loss_fn = nn.functional.binary_cross_entropy_with_logits\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "# losses=[]\n",
        "# epochs=25000\n",
        "# for epoch in range(epochs+1):\n",
        "#     optimizer.zero_grad() \n",
        "#     y_pred = model(X.float())  \n",
        "#     loss = loss_fn(y_pred, Y.float())\n",
        "#     loss.backward()  \n",
        "#     optimizer.step()  \n",
        "#     losses.append(loss.item())\n",
        "# plt.plot(losses)"
      ],
      "metadata": {
        "id": "_uDZIgrBWUJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T=torch.tensor(x_test.values)\n",
        "# model.eval()\n",
        "# prediction=model(T.float())>= torch.Tensor([0.5])\n",
        "# prediction=prediction.int()\n",
        "# print(prediction[:10])"
      ],
      "metadata": {
        "id": "-yd8a1v0Y2Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XYwHYqbfpk4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission['ProdTaken'] = prediction.detach().numpy()\n",
        "sample_submission.head()\n",
        "sample_submission.to_csv('submission.csv',index = False)"
      ],
      "metadata": {
        "id": "Gw2OyAmZjU4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}