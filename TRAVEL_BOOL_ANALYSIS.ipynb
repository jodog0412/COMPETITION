{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRAVEL_BOOL_ANALYSIS.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPc35lhJUB0MfEOqsmdTURN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jodog0412/DACON/blob/main/TRAVEL_BOOL_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "brUgcRpFxtsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBidw9kXDlVl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab/TRAVEL_PRODUCT_ANALYSIS\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 데이터 로드"
      ],
      "metadata": {
        "id": "LxQOy7oBfywn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "train = pd.read_csv('dataset/train.csv')\n",
        "test = pd.read_csv('dataset/test.csv')\n",
        "sample_submission = pd.read_csv('dataset/sample_submission.csv')\n",
        "train.head()\n",
        "# train.isna().sum()"
      ],
      "metadata": {
        "id": "v7GRxH_XD_Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 결측치 처리"
      ],
      "metadata": {
        "id": "Qzc18cuuf7ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_na = train.copy()\n",
        "test_na = test.copy()\n",
        "\n",
        "# 0 으로 채우는 경우\n",
        "train_na.DurationOfPitch = train_na.DurationOfPitch.fillna(0)\n",
        "test_na.DurationOfPitch=test_na.DurationOfPitch.fillna(0)\n",
        "\n",
        "\n",
        "# mean 값으로 채우는 경우\n",
        "mean_cols = ['Age',\n",
        "             'NumberOfFollowups',\n",
        "             'PreferredPropertyStar',\n",
        "             'NumberOfTrips',\n",
        "             'NumberOfChildrenVisiting',\n",
        "             'MonthlyIncome']\n",
        "             \n",
        "for col in mean_cols:\n",
        "    train_na[col] = train_na[col].fillna(train[col].mean())\n",
        "    test_na[col] = test_na[col].fillna(test[col].mean())\n",
        "\n",
        "# \"Unknown\"으로 채우는 경우\n",
        "train_na.TypeofContact = train_na.TypeofContact.fillna(\"Unknown\")\n",
        "test_na.TypeofContact = test_na.TypeofContact.fillna(\"Unknown\")"
      ],
      "metadata": {
        "id": "zTBjLGukevwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 문자열 변수 전처리"
      ],
      "metadata": {
        "id": "fEZqMds1gPEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = train.columns[train.dtypes == 'object']\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "train_enc = train_na.copy()\n",
        "test_enc=test_na.copy()\n",
        "for o_col in object_columns:\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(train_enc[o_col])\n",
        "    train_enc[o_col] = encoder.transform(train_enc[o_col])\n",
        "    test_enc[o_col] = encoder.transform(test_enc[o_col])\n",
        "\n",
        "train_enc.info()\n",
        "train_enc.describe(include=\"number\")\n",
        "# train_enc[\"MonthlyIncome\"].hist(bins=100)"
      ],
      "metadata": {
        "id": "fw-f_CB4EZ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 스케일링"
      ],
      "metadata": {
        "id": "odoHWUktgv5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "train_scale = train_enc.copy()\n",
        "test_scale=test_enc.copy()\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_scale[['Age',\n",
        "                        'DurationOfPitch',\n",
        "                        'MonthlyIncome']])\n",
        "train_scale[['Age','DurationOfPitch','MonthlyIncome']] = scaler.transform(train_scale[['Age', \n",
        "                                                                                       'DurationOfPitch', \n",
        "                                                                                       'MonthlyIncome']])\n",
        "test_scale[['Age', 'DurationOfPitch', 'MonthlyIncome']] = scaler.transform(test_scale[['Age',\n",
        "                                                                                       'DurationOfPitch',\n",
        "                                                                                       'MonthlyIncome']])\n",
        "# 결과를 확인합니다.\n",
        "train_scale.info()"
      ],
      "metadata": {
        "id": "cyUFHdbLgz5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 머신러닝"
      ],
      "metadata": {
        "id": "yzOEXFSnikMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) RandomForest"
      ],
      "metadata": {
        "id": "i8hPqmxSSR0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "x_train = train_scale.drop(columns=['id','ProdTaken'])\n",
        "y_train = train_scale[['ProdTaken']]\n",
        "x_test = test_scale.drop(columns=['id'])\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(x_train,y_train)\n",
        "prediction = model.predict(x_test)"
      ],
      "metadata": {
        "id": "Rg7MfQlPSZut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. DEEP LEARNING"
      ],
      "metadata": {
        "id": "e39Ui0EtZl96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) 데이터 처리"
      ],
      "metadata": {
        "id": "e2mMcx5i-aPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset # 학습 및 배치로 모델에 넣어주기 위한 툴\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "x_datas = train_scale.drop(columns=['id','ProdTaken'])\n",
        "y_datas = train_scale[['ProdTaken']]\n",
        "x_test = test_scale.drop(columns=['id'])\n",
        "x_datas.info()\n",
        "\n",
        "x_train, x_vali, y_train, y_vali = train_test_split(x_datas, \n",
        "                                                    y_datas, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "KKVsDYldzvV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'EPOCHS': 50, #에포크\n",
        "    'LEARNING_RATE':5e-3, #학습률\n",
        "    'BATCH_SIZE':16, #배치사이즈\n",
        "    'SEED':41, #시드\n",
        "}\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data, train_mode=True): #필요한 변수들을 선언\n",
        "        self.train_mode = train_mode\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        x = torch.FloatTensor(self.x_data.iloc[idx])\n",
        "        y = torch.FloatTensor(self.y_data.iloc[idx])\n",
        "        return x, y\n",
        "    \n",
        "train_dataset = CustomDataset(x_train, y_train, train_mode=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True)\n",
        "vali_dataset = CustomDataset(x_vali, y_vali, train_mode=True)\n",
        "vali_loader = DataLoader(vali_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False)\n"
      ],
      "metadata": {
        "id": "QuK7Ywxa-tDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) 모델링"
      ],
      "metadata": {
        "id": "yrb31RF_DwJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassification, self).__init__()\n",
        "        self.layer_1 = nn.Linear(18, 32) \n",
        "        self.layer_2 = nn.Linear(32, 64)\n",
        "        self.layer_3 = nn.Linear(64, 32)\n",
        "        self.layer_out = nn.Linear(32, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.relu(self.layer_3(x))\n",
        "        x = self.sigmoid(self.layer_out(x))\n",
        "        return x\n",
        "\n",
        "model=BinaryClassification().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=CFG['LEARNING_RATE'])\n",
        "loss_fn = F.binary_cross_entropy_with_logits\n",
        "\n",
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    return acc\n",
        "\n",
        "losses=[]\n",
        "accur=[]\n",
        "for epoch in range(CFG['EPOCHS']):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        y_pred = model(x_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        acc = binary_acc(y_pred, y_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        losses.append(epoch_loss/len(train_loader))\n",
        "        accur.append(epoch_acc/len(train_loader))\n",
        "    print('[{:d}] Train_Loss: {:.4f}'.format(epoch, epoch_loss/len(train_loader)))\n",
        "    print('[{:d}] Train_Accur: {:.4f}'.format(epoch, epoch_acc/len(train_loader)))\n",
        "\n",
        "    model.eval() #evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
        "    vali_loss = 0\n",
        "    vali_acc=0\n",
        "    with torch.no_grad(): #파라미터 업데이트 안하기 때문에 no_grad 사용\n",
        "        for x_batch, y_batch in vali_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(x_batch)\n",
        "            vali_loss += loss_fn(y_pred, y_batch).item()\n",
        "            vali_acc += binary_acc(y_pred, y_batch).item()\n",
        "    print('[{:d}] Vali_Accur: {:.4f}'.format(epoch, vali_acc/len(vali_loader)))\n",
        "plt.plot(accur)"
      ],
      "metadata": {
        "id": "_uDZIgrBWUJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0t5Qr0OAXki8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XYwHYqbfpk4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission['ProdTaken'] = prediction\n",
        "sample_submission.head()\n",
        "sample_submission.to_csv('submission.csv',index = False)"
      ],
      "metadata": {
        "id": "Gw2OyAmZjU4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0P3ut6TEthFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}